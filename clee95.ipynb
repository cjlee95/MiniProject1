{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written text as operational data\n",
    "\n",
    "Written text is one type of data\n",
    "\n",
    "### Why people write?\n",
    "\n",
    " - To communicate: their thoughts, feelings, urgency, needs, information\n",
    "\n",
    "### Why people communicate?\n",
    "\n",
    "1. To express emotions\n",
    "1. To share information\n",
    "1. To enable or elicit an action\n",
    "1. ...\n",
    "\n",
    "### We will use written text for the purpose other than \n",
    "1. To experience emotion\n",
    "1. To learn something the author intended us to learn\n",
    "1. To do what the author intended us to do\n",
    "\n",
    "### Instead, we will use written text to recognize who wrote it\n",
    " - By calculating and comparing word frequencies in written documents\n",
    " \n",
    "See, for example, likely fictional story https://medium.com/@amuse/how-the-nsa-caught-satoshi-nakamoto-868affcef595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. Dictionaries in python (associative arrays)\n",
    "\n",
    "Plot the frequency distribution of words on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE\t1\n",
      "PUBLIC\t1\n",
      "\"-//IETF//DTD\t1\n",
      "2.0//EN\">\t1\n",
      "<html><head>\t1\n",
      "<title>403\t1\n",
      "Forbidden</title>\t1\n",
      "</head><body>\t1\n",
      "<h1>Forbidden</h1>\t1\n",
      "<p>You\t1\n",
      "don't\t1\n",
      "have\t1\n",
      "permission\t1\n",
      "to\t1\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "# re is a module for regular expressions: to detect various combinations of characters\n",
    "import operator\n",
    "\n",
    "# Start from a simple document\n",
    "r = requests .get('http://eecs.utk.edu')\n",
    "\n",
    "# What comes back includes headers and other HTTP stuff, get just the body of the response\n",
    "t = r.text\n",
    "\n",
    "# obtain words by splitting a string using as separator one or more (+) space/like characters (\\s) \n",
    "wds = re.split('\\s+',t)\n",
    "\n",
    "# now populate a dictionary (wf)\n",
    "wf = {}\n",
    "for w in wds:\n",
    "    if w in wf: wf [w] = wf [w] + 1\n",
    "    else:  wf[w] = 1\n",
    "\n",
    "# dictionaries can not be sorted, so lets get a sorted *list*        \n",
    "wfs = sorted (wf .items(), key = operator .itemgetter (1), reverse=True)   \n",
    "\n",
    "# lets just have no more than 15 words \n",
    "ml = min(len(wfs),15)\n",
    "for i in range(1,ml,1):\n",
    "    print (wfs[i][0]+\"\\t\"+str(wfs[i][1]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Lots of markup in the output, lets remove it --- \n",
    "\n",
    "use BeautifulSoup and nltk modules and practice some regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import clean_html\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "# we may not care about the usage of stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english') + [\n",
    " 'ut', '\\'re','.', ',', '--', '\\'s', '?', ')', '(', ':', '\\'',\n",
    " '\\\"', '-', '}', '{', '&', '|', u'\\u2014' ]\n",
    "\n",
    "# We most likely would like to remove html markup\n",
    "def cleanHtml (html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup .get_text()\n",
    "\n",
    "# We also want to remove special characters, quotes, etc. from each word\n",
    "def cleanWord (w):\n",
    "    # r in r'[.,\"\\']' tells to treat \\ as a regular character \n",
    "    # but we need to escape ' with \\'\n",
    "    # any character between the brackets [] is to be removed \n",
    "    wn = re.sub('[,\"\\.\\'&\\|:@>*;/=]', \"\", w)\n",
    "    # get rid of numbers\n",
    "    return re.sub('^[0-9\\.]*$', \"\", wn)\n",
    "       \n",
    "# define a function to get text/clean/calculate frequency\n",
    "def get_wf (URL):\n",
    "    # first get the web page\n",
    "    r = requests .get(URL)\n",
    "    \n",
    "    # Now clean\n",
    "    # remove html markup\n",
    "    t = cleanHtml (r .text) .lower()\n",
    "    \n",
    "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
    "    wds = re .split('\\s+',t)\n",
    "    \n",
    "    # remove periods, commas, etc stuck to the edges of words\n",
    "    for i in range(len(wds)):\n",
    "        wds [i] = cleanWord (wds [i])\n",
    "    \n",
    "    # If satisfied with results, lets go to the next step: calculate frequencies\n",
    "    # We can write a loop to create a dictionary, but \n",
    "    # there is a special function for everything in python\n",
    "    # in particular for counting frequencies (like function table() in R)\n",
    "    wf = Counter (wds)\n",
    "    \n",
    "    # Remove stop words from the dictionary wf\n",
    "    for k in stop_words:\n",
    "        wf. pop(k, None)\n",
    "        \n",
    "    #how many regular words in the document?\n",
    "    tw = 0\n",
    "    for w in wf:\n",
    "       tw += wf[w] \n",
    "        \n",
    "    \n",
    "    # Get ordered list\n",
    "    wfs = sorted (wf .items(), key = operator.itemgetter(1), reverse=True)\n",
    "    ml = min(len(wfs),15)\n",
    "\n",
    "    #Reverse the list because barh plots items from the bottom\n",
    "    return (wfs [ 0:ml ] [::-1], tw)\n",
    "        \n",
    "# Now populate two lists    \n",
    "(wf_ee, tw_ee) = get_wf('http://www.gutenberg.org/ebooks/1342.txt.utf-8')\n",
    "(wf_bu, tw_bu) = get_wf('http://www.gutenberg.org/ebooks/76.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results: are there striking differences in language?\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "def plotTwoLists (wf_ee, wf_bu, title):\n",
    "    f = plt.figure (figsize=(10, 6))\n",
    "    # this is painfully tedious....\n",
    "    f .suptitle (title, fontsize=20)\n",
    "    ax = f.add_subplot(111)\n",
    "    ax .spines ['top'] .set_color ('none')\n",
    "    ax .spines ['bottom'] .set_color ('none')\n",
    "    ax .spines ['left'] .set_color ('none')\n",
    "    ax .spines ['right'] .set_color ('none')\n",
    "    ax .tick_params (labelcolor='w', top='off', bottom='off', left='off', right='off', labelsize=20)\n",
    "\n",
    "    # Create two subplots, this is the first one\n",
    "    ax1 = f .add_subplot (121)\n",
    "    plt .subplots_adjust (wspace=.5)\n",
    "\n",
    "    pos = np .arange (len(wf_ee)) \n",
    "    ax1 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pylab .yticks (pos, [ x [0] for x in wf_ee ])\n",
    "    ax1 .barh (range(len(wf_ee)), [ x [1] for x in wf_ee ], align='center')\n",
    "\n",
    "    ax2 = f .add_subplot (122)\n",
    "    ax2 .tick_params (axis='both', which='major', labelsize=14)\n",
    "    pos = np .arange (len(wf_bu)) \n",
    "    pylab .yticks (pos, [ x [0] for x in wf_bu ])\n",
    "    ax2 .barh (range (len(wf_bu)), [ x [1] for x in wf_bu ], align='center')\n",
    "\n",
    "plotTwoLists (wf_ee, wf_bu, 'Difference between Pride and Prejudice and Huck Finn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case Project gutenberg is blocked you can download text to your laptop and copy to the docker container via scp\n",
    "#Assuming the file name you copy is pg4680.txt here is how you change the script\n",
    "# Please note the option errors='replace'\n",
    "# without it python invariably runs into unicode errors\n",
    "f = open ('pg4680.txt', 'r', encoding=\"ascii\", errors='replace')\n",
    "    \n",
    "# What comes back includes headers and other HTTP stuff, get just the body of the response\n",
    "t = f.read()\n",
    "\n",
    "# obtain words by splitting a string using as separator one or more (+) space/like characters (\\s) \n",
    "wds = re.split('\\s+',t)\n",
    "\n",
    "# now populate a dictionary (wf)\n",
    "wf = {}\n",
    "for w in wds:\n",
    "    if w in wf: wf [w] = wf [w] + 1\n",
    "    else:  wf [w] = 1\n",
    "\n",
    "# dictionaries can not be sorted, so lets get a sorted *list*        \n",
    "wfs = sorted (wf .items(), key = operator .itemgetter (1), reverse=True)   \n",
    "\n",
    "# lets just have no more than 15 words \n",
    "ml = min(len(wfs),15)\n",
    "for i in range(1,ml,1):\n",
    "    print (wfs[i][0]+\"\\t\"+str(wfs[i][1]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "1. Compare word frequencies between two works of a single author.\n",
    "1. Compare word frequencies between works of two authors.\n",
    "1. Are there some words preferred by one author but used less frequently by another author?\n",
    "\n",
    "Extra credit\n",
    "\n",
    "1. The frequency of a specific word, e.g., \"would\" should follow a binomial distribution (each regular word in a document is a trial and with probability p that word is \"would\". The estimate for p is N(\"would\")/N(regular word)). Do these binomial distributions for your chosen word differ significantly between books of the same author or between authors? \n",
    "\n",
    "Project Gutenberg is a good source of for fiction and non-fiction.\n",
    "\n",
    "E.g below are two most popular books from Project Gutenberg:\n",
    "- Pride and Prejudice at http://www.gutenberg.org/ebooks/1342.txt.utf-8\n",
    "- Adventures of Huckleberry Finn at http://www.gutenberg.org/ebooks/76.txt.utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk\n",
    "#In case your text is not on Project Gutenberg but at some other URL\n",
    "#http://www.fullbooks.com/Our-World-or-The-Slaveholders-Daughter2.html\n",
    "# that contains 12 parts\n",
    "t = \"\"\n",
    "for i in range(2,13):\n",
    "  r = requests .get('http://www.fullbooks.com/Our-World-or-The-Slaveholders-Daughter' + str(i) + '.html')\n",
    "  t = t + r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1323653"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Iliad:\n",
      "{'the': 7019, 'and': 5326, 'of': 3636, 'to': 3577, 'a': 2033, 'in': 1916, 'i': 1896, 'he': 1878, 'you': 1862, 'for': 1375, 'his': 1342, 'as': 1292, 'that': 1285, 'with': 1244, 'it': 1227, 'was': 1063, 'him': 1033, 'they': 962, 'is': 959, 'on': 915, 'had': 889, 'have': 878, 'me': 866, 'all': 866, 'but': 853, 'my': 817, 'not': 791, 'them': 758, 'will': 730, 'so': 689}\n",
      "This is the Odyssey:\n",
      "{'the': 10153, 'and': 6674, 'of': 5760, 'to': 3397, 'he': 2887, 'his': 2540, 'in': 2294, 'a': 1850, 'him': 1830, 'you': 1791, 'with': 1737, 'that': 1454, 'for': 1395, 'as': 1393, 'i': 1280, 'was': 1220, 'they': 1135, 'it': 1131, 'on': 1102, 'them': 1029, 'from': 1005, 'but': 994, 'son': 959, 'not': 915, 'had': 892, 'their': 872, 'all': 822, 'is': 818, 'will': 800, 'by': 764}\n",
      "This is Moby Dick:\n",
      "{'the': 14526, 'of': 6713, 'and': 6404, 'a': 4665, 'to': 4650, 'in': 4184, 'that': 2895, 'his': 2515, 'it': 2280, 'i': 1842, 'with': 1763, 'but': 1740, 'as': 1721, 'is': 1717, 'he': 1711, 'was': 1636, 'for': 1606, 'all': 1462, 'this': 1392, 'at': 1317, 'by': 1215, 'not': 1140, 'from': 1103, 'be': 1050, 'on': 1040, 'so': 1024, 'him': 993, 'you': 913, 'whale': 897, 'one': 883}\n"
     ]
    }
   ],
   "source": [
    "import requests, re, nltk\n",
    "import itertools \n",
    "\n",
    "\n",
    "t = \"\"\n",
    "r = requests.get('https://www.gutenberg.org/files/1727/1727-h/1727-h.htm')\n",
    "t = t + r.text\n",
    "\n",
    "t = cleanHtml (r .text) .lower()\n",
    "\n",
    "wds = re .split('\\s+',t)\n",
    "\n",
    "for i in range(len(wds)):\n",
    "    wds [i] = cleanWord (wds [i])\n",
    "    \n",
    "wf = {}\n",
    "for w in wds:\n",
    "    if w in wf: wf [w] = wf [w] + 1\n",
    "    else:  wf[w] = 1\n",
    "        \n",
    "wfs = sorted (wf .items(), key = operator .itemgetter (1), reverse=True)   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "t1 = \"\"\n",
    "r1 = requests.get('https://www.gutenberg.org/files/2199/2199-h/2199-h.htm')\n",
    "t1 = t1 + r1.text\n",
    "\n",
    "t1 = cleanHtml (r1 .text) .lower()\n",
    "\n",
    "wds1 = re .split('\\s+',t1)\n",
    "\n",
    "for i in range(len(wds1)):\n",
    "    wds1 [i] = cleanWord (wds1 [i])\n",
    "    \n",
    "wf1 = {}\n",
    "for w in wds1:\n",
    "    if w in wf1: wf1 [w] = wf1 [w] + 1\n",
    "    else:  wf1[w] = 1\n",
    "        \n",
    "wfs1 = sorted (wf1 .items(), key = operator .itemgetter (1), reverse=True)   \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "t2 = \"\"\n",
    "r2 = requests.get('https://www.gutenberg.org/files/2701/2701-h/2701-h.htm')\n",
    "t2 = t2 + r2.text\n",
    "\n",
    "t2 = cleanHtml (r2 .text) .lower()\n",
    "\n",
    "wds2 = re .split('\\s+',t2)\n",
    "for i in range(len(wds2)):\n",
    "    wds2 [i] = cleanWord (wds2 [i])\n",
    "    \n",
    "wf2 = {}\n",
    "for w in wds2:\n",
    "    if w in wf2: wf2 [w] = wf2 [w] + 1\n",
    "    else:  wf2[w] = 1\n",
    "\n",
    "wfs2 = sorted (wf2 .items(), key = operator .itemgetter (1), reverse=True)   \n",
    "\n",
    "        \n",
    "print(\"This is the Iliad:\")\n",
    "out = dict(itertools.islice(wfs, 30)) \n",
    "print(out)\n",
    "\n",
    "print(\"This is the Odyssey:\")\n",
    "out1 = dict(itertools.islice(wfs1, 30)) \n",
    "print(out1)\n",
    "\n",
    "\n",
    "print(\"This is Moby Dick:\")\n",
    "out2 = dict(itertools.islice(wfs2, 30)) \n",
    "print(out2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iliad and Odysseys' most used words are \"the\" and this is most likely due to the wording of the wording at the time and the translation that took place. As both of these have proper nouns for names and places so it would make sense to have the word \"the\" as the most common. As for Moby Dick, the most common word is \"the\" as well and it is evident that \"the\" is a common word and that the authors both use common words. Perhaps manually removing these words would bring out more unique words for better comparison. Overall, both authors, Homer and Herman Melville used similar words and would most likely use more unique words without the prepositions. Such as Herman using the word \"whale\" 897 times which is unique to Moby Dick while Homer uses the word \"son\" which is unique to the Odyssey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
